import os
import time
import redis
import numpy as np
import streamlit as st
from dotenv import load_dotenv
from redis.commands.search.query import Query

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI

# --- C√°c h·∫±ng s·ªë v√† c·∫•u h√¨nh ---
TOP_K = 3
SIMILARITY_THRESHOLD = 0.3
EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'
EMBEDDING_DIM = 384
REDIS_INDEX_NAME = "question_cache_idx_v3"

st.set_page_config(page_title="RAG + Semantic Cache", page_icon="üß†")

load_dotenv()
MY_GEMINI_API_KEY = os.getenv("MY_GEMINI_API_KEY")
if not MY_GEMINI_API_KEY:
    st.error("Vui l√≤ng thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng MY_GEMINI_API_KEY.")
    st.stop()

try:
    r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=False)
    r.ping()
except redis.exceptions.ConnectionError as e:
    st.error(f"L·ªói k·∫øt n·ªëi Redis: {e}. H√£y ƒë·∫£m b·∫£o Redis server ƒëang ch·∫°y.")
    st.stop()

@st.cache_resource
def load_embedding_model():
    return HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
embeddings = load_embedding_model()

@st.cache_resource
def load_llm():
    return ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0.3,
        google_api_key=MY_GEMINI_API_KEY,
    )
llm = load_llm()

def create_redis_index_alternative():
    try:
        r.ft(REDIS_INDEX_NAME).info()
    except redis.exceptions.ResponseError:
        st.sidebar.write(f"ƒêang t·∫°o index '{REDIS_INDEX_NAME}'...")
        command = [
            "FT.CREATE", REDIS_INDEX_NAME, "ON", "HASH",
            "PREFIX", "1", "q:", "SCHEMA",
            "question", "TEXT", "short_answer", "TEXT",
            "embedding", "VECTOR", "FLAT", "6",
            "TYPE", "FLOAT32", "DIM", EMBEDDING_DIM, "DISTANCE_METRIC", "COSINE"
        ]
        try:
            r.execute_command(*[str(item) for item in command])
        except Exception as e:
            st.error(f"L·ªói khi t·∫°o index Redis: {e}")
            return False
    return True

INDEX_READY = create_redis_index_alternative()

def check_cache(question: str):
    if not INDEX_READY: return None
    try:
        query_vector_bytes = np.array(embeddings.embed_query(question), dtype=np.float32).tobytes()
        redis_query = (
            Query(f"*=>[KNN {TOP_K} @embedding $vec AS score]")
            .sort_by("score")
            .return_fields("short_answer", "score")
            .dialect(2)
        )
        res = r.ft(REDIS_INDEX_NAME).search(redis_query, {"vec": query_vector_bytes}).docs
        if not res: return None

        best_match = res[0]
        if float(best_match.score) <= SIMILARITY_THRESHOLD:
            st.sidebar.info(f"Cache Hit! Score: {float(best_match.score):.4f}")
            return best_match.short_answer

        st.sidebar.warning(f"Cache Miss. G·∫ßn nh·∫•t: {float(best_match.score):.4f}")
        return None
    except Exception as e:
        print(f"L·ªói khi ki·ªÉm tra cache: {e}")
        return None

def get_short_answer_from_context(retriever, question: str) -> str:
    template = "D·ª±a tr√™n t√†i li·ªáu sau, vi·∫øt m·ªôt b·∫£n t√≥m t·∫Øt s√∫c t√≠ch ch·ª©a th√¥ng tin c·∫ßn thi·∫øt ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi(6 t·ª´ t·ªëi ƒëa).\nT√†i li·ªáu: {context}\nC√¢u h·ªèi: {question}\nT√≥m t·∫Øt:"
    prompt = PromptTemplate.from_template(template)
    chain = ({"context": retriever, "question": RunnablePassthrough()} | prompt | llm | StrOutputParser())
    return chain.invoke(question)

def get_short_answer_from_general_knowledge(question: str) -> str:
    template = "H√£y t·∫°o m·ªôt b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn c√°c th√¥ng tin tr·ªçng y·∫øu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi sau(6 t·ª´ t·ªëi ƒëa).\nC√¢u h·ªèi: {question}\nT√≥m t·∫Øt:"
    prompt = PromptTemplate.from_template(template)
    chain = prompt | llm | StrOutputParser()
    return chain.invoke({"question": question})

def get_final_answer(context: str, question: str) -> str:
    template = "D·ª±a v√†o th√¥ng tin sau, tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch t·ª± nhi√™n.\nTh√¥ng tin: {context}\nC√¢u h·ªèi: {question}\nC√¢u tr·∫£ l·ªùi:"
    prompt = PromptTemplate.from_template(template)
    chain = prompt | llm | StrOutputParser()
    return chain.invoke({"context": context, "question": question})

def save_to_cache(question: str, short_answer: str):
    if not INDEX_READY:
        return
    try:
        embedding_vector = np.array(embeddings.embed_query(question), dtype=np.float32)
        vector_bytes = embedding_vector.tobytes()
        doc_id = f"q:{hash(question)}"
        r.hset(doc_id, mapping={
            b"question": question.encode("utf-8"),
            b"short_answer": short_answer.encode("utf-8"),
            b"embedding": vector_bytes
        })
    except Exception as e:
        st.error(f"L·ªói khi l∆∞u v√†o cache: {e}")

# --- Giao di·ªán Streamlit ---
st.title("Tr·ª£ l√Ω AI v·ªõi Semantic Cache")
st.caption("M√¥ ph·ªèng h·ªá th·ªëng t·ª´ b√†i b√°o 'Semantic Caching of Contextual Summaries'")

if 'messages' not in st.session_state: st.session_state.messages = []
if 'retriever' not in st.session_state: st.session_state.retriever = None

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

with st.sidebar:
    st.header("Ch·∫ø ƒë·ªô RAG")
    uploaded_file = st.file_uploader("T·∫£i l√™n file PDF (t√πy ch·ªçn)", type="pdf")
    if uploaded_file and st.button("X·ª≠ l√Ω t√†i li·ªáu"):
        with st.spinner("ƒêang x·ª≠ l√Ω PDF..."):
            with open("temp.pdf", "wb") as f: f.write(uploaded_file.read())
            docs = PyPDFLoader("temp.pdf").load()
            splits = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100).split_documents(docs)
            vectorstore = FAISS.from_documents(splits, embeddings)
            st.session_state.retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        st.success("T√†i li·ªáu ƒë√£ s·∫µn s√†ng!")

with st.sidebar.expander("Xem cache g·∫ßn ƒë√¢y"):
    keys = r.keys("q:*")
    for k in keys[-5:]:
        st.markdown(f"- `{r.hget(k, 'question')}`")

# --- Lu·ªìng x·ª≠ l√Ω ch√≠nh ---
if prompt := st.chat_input(" Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        if not INDEX_READY:
            st.error("L·ªói: Index c·ªßa Redis kh√¥ng s·∫µn s√†ng.")
            st.stop()

        with st.spinner("ƒêang suy nghƒ©..."):
            cached_content = check_cache(prompt)

            if cached_content:
                answer = get_final_answer(cached_content, prompt)
                st.info("C√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c t·∫°o t·ª´ cache.", icon="‚ö°")
            else:
                st.warning("Kh√¥ng t√¨m th·∫•y trong cache. ƒêang t·∫°o m·ªõi...")
                if st.session_state.retriever is not None:
                    short_answer = get_short_answer_from_context(st.session_state.retriever, prompt)
                else:
                    short_answer = get_short_answer_from_general_knowledge(prompt)

                answer = get_final_answer(short_answer, prompt)
                save_to_cache(prompt, short_answer)

        st.markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})